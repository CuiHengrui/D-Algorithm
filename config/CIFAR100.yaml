clients:
    # Type
    type: simple

    # The total number of clients
    total_clients: 20

    # The number of clients selected in each round
    per_round: 20

    # Should the clients compute test accuracy locally?
    do_test: false

    # Processors for outbound data payloads

    bandwidth:
    # - 7
    # - 19
    # - 16
    # - 5
    # - 8
    # - 17
    # - 13
    # - 10
    # - 11
    # - 14
    # - 19
    # - 18
    # - 16
    # - 14
    # - 12
    # - 12
    # - 10
    # - 8
    # - 6
    # - 5

    - 5.6
    - 15.2
    - 12.8
    - 4
    - 6.4
    - 13.6
    - 10.4
    - 8
    - 8.8
    - 11.2
    - 15.2
    - 14.4
    - 12.8
    - 11.2
    - 9.6
    - 9.6
    - 8
    - 6.4
    - 4.8
    - 4

    cpu_freq:
    - 3.2
    - 3.0
    - 2.4
    - 3.8
    - 2.2
    - 3.6
    - 2.8
    - 2.0
    - 3.4
    - 2.6
    - 3.2
    - 4.2
    - 1.2
    - 4.9
    - 1.6
    - 1.8
    - 2.4
    - 4.0
    - 1.7
    - 4.3

    cn:
    - 10
    - 22
    - 16
    - 18
    - 14
    - 20
    - 28
    - 24
    - 12
    - 26
    - 10
    - 22
    - 16
    - 18
    - 14
    - 20
    - 28
    - 24
    - 12
    - 26

    # 最大cpu频率，单位MHZ
    max_cpu_freq: 2000

    # 最小cpu频率
    min_cpu_freq: 1000

    # 单个大小
    sample_size: 1

server:
    address: 127.0.0.1
    port: 18001
    # random_seed: 1
    simulate_wall_time: true

    s_init: 128
    do_test: true

    # Processors for inbound data payloads

data:
    # The training and testing dataset
    datasource: CIFAR100

    # Where the dataset is located
    data_path: ./data

    # Number of samples in each partition
    min_partition_size: 2000

    # IID or non-IID?
    sampler: fixed_noniid

trainer:
    # The type of the trainer
    type: basic

    # The maximum number of training rounds
    rounds: 200

    # The maximum number of clients running concurrently
    max_concurrency: 5

    # The target accuracy
    target_accuracy: 0.95

    # The machine learning model
    model_name: resnet_152


    # Number of epoches for local training in each communication round
    epochs: 5
    batch_size: 64
    optimizer: SGD
    lr_scheduler: StepLR

algorithm:
    type: fedavg
    solver: greedy
    alpha: 0.6
    B: 16


parameters:
    model:
        num_classes: 100
    optimizer:
        lr: 0.001
        momentum: 0.9
        weight_decay: 0.0001

    learning_rate:
        step_size: 30
        gamma: 0.1

#parameters:
#    model:
#        num_classes: 200
#    optimizer:
#        lr: 0.001
#        momentum: 0.9
#        weight_decay: 0.0001
#
#    learning_rate:
#        step_size: 30
#        gamma: 0.1
